#!/usr/bin/env python3
"""
üöÄ BM25 INDEX BUILDER
–°–æ–∑–¥–∞–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞ —Å –ø–æ–º–æ—â—å—é Tantivy –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –ø–æ–∏—Å–∫–∞
"""

import json
from pathlib import Path
from typing import List, Dict
import re
import math
from collections import defaultdict, Counter

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –∏–º–ø–æ—Ä—Ç tantivy
try:
    import tantivy
    TANTIVY_AVAILABLE = True
except ImportError:
    TANTIVY_AVAILABLE = False
    print("‚ö†Ô∏è tantivy –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –ø—Ä–æ—Å—Ç–∞—è TF-IDF –∏–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü–∏—è")

def clean_text_for_bm25(text: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è BM25 –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è"""
    # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤–∞–∂–Ω—ã–µ –¥–ª—è –∫–æ–¥–∞
    text = re.sub(r'[^\w\s\.\(\)\[\]{}=+\-*/<>!&|^%]', ' ', text)
    # –£–±–∏—Ä–∞–µ–º –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

class SimpleBM25:
    """–ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è BM25 –±–µ–∑ Tantivy"""
    
    def __init__(self, documents: List[Dict], k1: float = 1.2, b: float = 0.75):
        self.documents = documents
        self.k1 = k1
        self.b = b
        
        # –ü—Ä–µ–¥–æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.processed_docs = []
        self.doc_freqs = defaultdict(int)
        self.idf = {}
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        total_len = 0
        for doc in documents:
            cleaned_text = clean_text_for_bm25(doc["text"])
            tokens = cleaned_text.lower().split()
            self.processed_docs.append({
                "id": doc["id"],
                "tokens": tokens,
                "doc": doc,
                "token_count": Counter(tokens)
            })
            total_len += len(tokens)
            
            # –°—á–∏—Ç–∞–µ–º —á–∞—Å—Ç–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–µ—Ä–º–∏–Ω–∞
            for token in set(tokens):
                self.doc_freqs[token] += 1
        
        self.avg_doc_len = total_len / len(documents) if documents else 0
        
        # –í—ã—á–∏—Å–ª—è–µ–º IDF
        N = len(documents)
        for term, df in self.doc_freqs.items():
            self.idf[term] = math.log((N - df + 0.5) / (df + 0.5))
    
    def search(self, query: str, limit: int = 10) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ BM25"""
        query_tokens = clean_text_for_bm25(query).lower().split()
        scores = []
        
        for proc_doc in self.processed_docs:
            score = 0.0
            doc_len = len(proc_doc["tokens"])
            
            for term in query_tokens:
                if term in proc_doc["token_count"]:
                    tf = proc_doc["token_count"][term]
                    idf = self.idf.get(term, 0)
                    
                    # BM25 —Ñ–æ—Ä–º—É–ª–∞
                    numerator = idf * tf * (self.k1 + 1)
                    denominator = tf + self.k1 * (1 - self.b + self.b * (doc_len / self.avg_doc_len))
                    score += numerator / denominator
            
            if score > 0:
                result = proc_doc["doc"].copy()
                result["bm25_score"] = score
                scores.append(result)
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å–∫–æ—Ä–∞
        scores.sort(key=lambda x: x["bm25_score"], reverse=True)
        return scores[:limit]

def build_bm25_index(documents_path: str, output_dir: str = "rag_data") -> bool:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    
    documents_path_obj = Path(documents_path)
    output_dir_obj = Path(output_dir)
    output_dir_obj.mkdir(exist_ok=True)
    
    print(f"üîç –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ BM25 –∏–Ω–¥–µ–∫—Å–∞ –∏–∑: {documents_path_obj}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    if not documents_path_obj.exists():
        print(f"‚ùå –§–∞–π–ª –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω: {documents_path_obj}")
        return False
    
    with open(documents_path_obj, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    
    print(f"üìÑ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    
    if TANTIVY_AVAILABLE:
        return _build_tantivy_index(documents, output_dir_obj)
    else:
        return _build_simple_index(documents, output_dir_obj)

def _build_tantivy_index(documents: List[Dict], output_dir: Path) -> bool:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ —Å –ø–æ–º–æ—â—å—é Tantivy"""
    
    # –°–æ–∑–¥–∞–µ–º —Å—Ö–µ–º—É –∏–Ω–¥–µ–∫—Å–∞
    schema_builder = tantivy.SchemaBuilder()
    
    # –ü–æ–ª—è –∏–Ω–¥–µ–∫—Å–∞
    schema_builder.add_text_field("id", stored=True, tokenizer_name="raw")
    schema_builder.add_text_field("file_path", stored=True, tokenizer_name="raw") 
    schema_builder.add_text_field("chunk_type", stored=True, tokenizer_name="raw")
    schema_builder.add_text_field("text", stored=True, tokenizer_name="default")
    schema_builder.add_text_field("text_cleaned", tokenizer_name="default")  # –î–ª—è –ø–æ–∏—Å–∫–∞
    schema_builder.add_integer_field("chunk_size", stored=True)
    
    schema = schema_builder.build()
    
    # –°–æ–∑–¥–∞–µ–º –∏–Ω–¥–µ–∫—Å
    index_path = output_dir / "bm25_index"
    if index_path.exists():
        import shutil
        shutil.rmtree(index_path)
    
    index = tantivy.Index(schema, path=str(index_path))
    writer = index.writer(heap_size=50_000_000)  # 50MB heap
    
    print("‚öôÔ∏è –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    
    # –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
    for doc in documents:
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞
        cleaned_text = clean_text_for_bm25(doc["text"])
        
        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –≤ –∏–Ω–¥–µ–∫—Å
        writer.add_document(tantivy.Document(
            id=str(doc["id"]),
            file_path=doc["file_path"],
            chunk_type=doc["chunk_type"],
            text=doc["text"],
            text_cleaned=cleaned_text,
            chunk_size=doc["chunk_size"]
        ))
    
    # –ö–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
    writer.commit()
    print(f"‚úÖ BM25 –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {index_path}")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞...")
    reader = index.reader()
    searcher = reader.searcher()
    
    # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    query_parser = tantivy.QueryParser.for_index(index, ["text_cleaned"])
    test_query = query_parser.parse_query("function")
    
    top_docs = searcher.search(test_query, 5)
    print(f"   –ù–∞–π–¥–µ–Ω–æ {len(top_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∑–∞–ø—Ä–æ—Å—É 'function'")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å–∞
    bm25_metadata = {
        "index_type": "tantivy",
        "index_path": str(index_path),
        "total_documents": len(documents),
        "fields": ["id", "file_path", "chunk_type", "text", "text_cleaned", "chunk_size"],
        "tokenizer": "default",
        "test_query_results": len(top_docs)
    }
    
    with open(output_dir / "bm25_metadata.json", "w", encoding="utf-8") as f:
        json.dump(bm25_metadata, f, ensure_ascii=False, indent=2)
    
    print("üíæ BM25 –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    return True

def _build_simple_index(documents: List[Dict], output_dir: Path) -> bool:
    """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ BM25 –∏–Ω–¥–µ–∫—Å–∞"""
    
    print("‚öôÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–≥–æ BM25 –∏–Ω–¥–µ–∫—Å–∞...")
    
    # –°–æ–∑–¥–∞–µ–º BM25 –æ–±—ä–µ–∫—Ç
    bm25 = SimpleBM25(documents)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
    index_data = {
        "documents": documents,
        "processed_docs": bm25.processed_docs,
        "doc_freqs": dict(bm25.doc_freqs),
        "idf": bm25.idf,
        "avg_doc_len": bm25.avg_doc_len,
        "k1": bm25.k1,
        "b": bm25.b
    }
    
    with open(output_dir / "simple_bm25_index.json", "w", encoding="utf-8") as f:
        json.dump(index_data, f, ensure_ascii=False, indent=2)
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    test_results = bm25.search("function", 5)
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    bm25_metadata = {
        "index_type": "simple_bm25",
        "index_path": str(output_dir / "simple_bm25_index.json"),
        "total_documents": len(documents),
        "avg_doc_len": bm25.avg_doc_len,
        "vocab_size": len(bm25.idf),
        "test_query_results": len(test_results)
    }
    
    with open(output_dir / "bm25_metadata.json", "w", encoding="utf-8") as f:
        json.dump(bm25_metadata, f, ensure_ascii=False, indent=2)
    
    print(f"‚úÖ –ü—Ä–æ—Å—Ç–æ–π BM25 –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω: {output_dir / 'simple_bm25_index.json'}")
    print("üíæ BM25 –∏–Ω–¥–µ–∫—Å –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!")
    return True

def search_bm25(query: str, index_path: str, limit: int = 10) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –≤ BM25 –∏–Ω–¥–µ–∫—Å–µ"""
    index_path_obj = Path(index_path)
    
    if TANTIVY_AVAILABLE and (index_path_obj / "meta.json").exists():
        return _search_tantivy(query, index_path_obj, limit)
    else:
        # –ò—â–µ–º –ø—Ä–æ—Å—Ç–æ–π –∏–Ω–¥–µ–∫—Å
        simple_index_path = index_path_obj / "simple_bm25_index.json"
        if simple_index_path.exists():
            return _search_simple(query, simple_index_path, limit)
        else:
            print(f"‚ùå BM25 –∏–Ω–¥–µ–∫—Å –Ω–µ –Ω–∞–π–¥–µ–Ω: {index_path_obj}")
            return []

def _search_tantivy(query: str, index_path: Path, limit: int) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –≤ Tantivy –∏–Ω–¥–µ–∫—Å–µ"""
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å
        schema_builder = tantivy.SchemaBuilder()
        schema_builder.add_text_field("id", stored=True, tokenizer_name="raw")
        schema_builder.add_text_field("file_path", stored=True, tokenizer_name="raw")
        schema_builder.add_text_field("chunk_type", stored=True, tokenizer_name="raw") 
        schema_builder.add_text_field("text", stored=True, tokenizer_name="default")
        schema_builder.add_text_field("text_cleaned", tokenizer_name="default")
        schema_builder.add_integer_field("chunk_size", stored=True)
        schema = schema_builder.build()
        
        index = tantivy.Index(schema, path=str(index_path))
        reader = index.reader()
        searcher = reader.searcher()
        
        # –°–æ–∑–¥–∞–µ–º –∑–∞–ø—Ä–æ—Å
        query_parser = tantivy.QueryParser.for_index(index, ["text_cleaned"])
        parsed_query = query_parser.parse_query(query)
        
        # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        top_docs = searcher.search(parsed_query, limit)
        
        results = []
        for score, doc_address in top_docs:
            doc = searcher.doc(doc_address)
            results.append({
                "id": doc["id"][0],
                "file_path": doc["file_path"][0],
                "chunk_type": doc["chunk_type"][0],
                "text": doc["text"][0],
                "chunk_size": doc["chunk_size"][0],
                "bm25_score": score
            })
        
        return results
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ Tantivy: {e}")
        return []

def _search_simple(query: str, index_path: Path, limit: int) -> List[Dict]:
    """–ü–æ–∏—Å–∫ –≤ –ø—Ä–æ—Å—Ç–æ–º BM25 –∏–Ω–¥–µ–∫—Å–µ"""
    try:
        with open(index_path, 'r', encoding='utf-8') as f:
            index_data = json.load(f)
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º BM25 –æ–±—ä–µ–∫—Ç
        bm25 = SimpleBM25(index_data["documents"])
        bm25.processed_docs = index_data["processed_docs"]
        bm25.doc_freqs = defaultdict(int, index_data["doc_freqs"])
        bm25.idf = index_data["idf"]
        bm25.avg_doc_len = index_data["avg_doc_len"]
        bm25.k1 = index_data["k1"]
        bm25.b = index_data["b"]
        
        # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Counter –æ–±—ä–µ–∫—Ç—ã
        for proc_doc in bm25.processed_docs:
            proc_doc["token_count"] = Counter(proc_doc["token_count"])
        
        return bm25.search(query, limit)
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –≤ –ø—Ä–æ—Å—Ç–æ–º BM25: {e}")
        return []

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:")
        print("  –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ: python build_bm25.py build <documents.json> [output_dir]")
        print("  –ü–æ–∏—Å–∫: python build_bm25.py search <query> <index_path> [limit]")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "build":
        if len(sys.argv) < 3:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É documents.json")
            sys.exit(1)
        
        documents_path = sys.argv[2]
        output_dir = sys.argv[3] if len(sys.argv) > 3 else "rag_data"
        
        success = build_bm25_index(documents_path, output_dir)
        if success:
            print("üéâ BM25 –∏–Ω–¥–µ–∫—Å –ø–æ—Å—Ç—Ä–æ–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        else:
            print("‚ùå –û—à–∏–±–∫–∞ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è BM25 –∏–Ω–¥–µ–∫—Å–∞")
            sys.exit(1)
    
    elif command == "search":
        if len(sys.argv) < 4:
            print("‚ùå –£–∫–∞–∂–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –∏ –ø—É—Ç—å –∫ –∏–Ω–¥–µ–∫—Å—É")
            sys.exit(1)
        
        query = sys.argv[2]
        index_path = sys.argv[3]
        limit = int(sys.argv[4]) if len(sys.argv) > 4 else 10
        
        results = search_bm25(query, index_path, limit)
        
        print(f"üîç –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ '{query}':")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result['file_path']} (score: {result['bm25_score']:.3f})")
            print(f"   Type: {result['chunk_type']}")
            print(f"   Text: {result['text'][:100]}...")
            print()
    
    else:
        print("‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –∫–æ–º–∞–Ω–¥–∞. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ 'build' –∏–ª–∏ 'search'")
        sys.exit(1) 