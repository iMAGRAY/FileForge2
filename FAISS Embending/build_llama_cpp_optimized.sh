#!/bin/bash
# ğŸš€ OPTIMIZED LLAMA.CPP BUILD FOR RTX 4070 + i7-14700
# Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸

set -e

echo "ğŸš€ BUILDING OPTIMIZED LLAMA.CPP FOR RTX 4070"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "ğŸ’» CPU: i7-14700 (20 cores P-E Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½Ğ½Ğ°Ñ Ğ°Ñ€Ñ…Ğ¸Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ°)"
echo "ğŸ® GPU: RTX 4070 (12GB VRAM, Ampere CC 8.6)"
echo "ğŸ’¾ RAM: 100GB Monster Configuration"
echo "âš¡ Target: 35-37 t/s, embedding 10M vec/s, first token <180ms"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Check prerequisites
echo ""
echo "ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ½Ñ‹Ñ… Ñ‚Ñ€ĞµĞ±Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹..."

# Check CUDA (ÑĞ¾Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğ¹ ÑĞ¿Ğ¾ÑĞ¾Ğ± Ğ±ĞµĞ· nvcc)
if command -v cuda-compiler &> /dev/null; then
    CUDA_VERSION=$(cuda-compiler --version 2>/dev/null | grep -o '[0-9]\+\.[0-9]\+' | head -1)
    if [[ -n "$CUDA_VERSION" ]]; then
        echo "âœ… CUDA Version: $CUDA_VERSION (cuda-compiler)"
    fi
elif command -v nvcc &> /dev/null; then
    CUDA_VERSION=$(nvcc --version | grep "release" | sed 's/.*release \([0-9]\+\.[0-9]\+\).*/\1/')
    echo "âœ… CUDA Version: $CUDA_VERSION (nvcc fallback)"
else
    echo "âŒ CUDA toolkit Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½! Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ CUDA 12.4+"
    exit 1
fi

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ²ĞµÑ€ÑĞ¸Ğ¸ CUDA Ğ´Ğ»Ñ RTX 4070
if [[ $(printf '%s\n' "12.4" "$CUDA_VERSION" | sort -V | head -n1) != "12.4" ]]; then
    echo "âš ï¸  Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ CUDA 12.4+ Ğ´Ğ»Ñ RTX 4070, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½: $CUDA_VERSION"
fi

# Check CMAKE (Ğ¼Ğ¸Ğ½Ğ¸Ğ¼ÑƒĞ¼ 3.21+ Ğ´Ğ»Ñ RTX 4070 cc 8.6)
if ! command -v cmake &> /dev/null; then
    echo "âŒ CMAKE Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½! Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ cmake 3.21+"
    exit 1
fi

cmake_min=$(cmake --version | awk 'NR==1{print $3}')
echo "âœ… CMAKE Version: $cmake_min"

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ²ĞµÑ€ÑĞ¸Ğ¸ CMake 3.21+ Ğ´Ğ»Ñ Ğ¸Ğ·Ğ±ĞµĞ¶Ğ°Ğ½Ğ¸Ñ Ğ±Ğ°Ğ³Ğ¾Ğ² Ñ cublasLt header
if [[ $(printf '%s\n' "3.21" "$cmake_min" | sort -V | head -n1) != "3.21" ]]; then
    echo "âŒ CMake â‰¥3.21 Ğ½ÑƒĞ¶ĞµĞ½ Ğ´Ğ»Ñ RTX 4070 (cc 8.6) Ğ¸Ğ·-Ğ·Ğ° FindCUDAToolkit"
    echo "   Ğ¢ĞµĞºÑƒÑ‰Ğ°Ñ Ğ²ĞµÑ€ÑĞ¸Ñ: $cmake_min"
    exit 1
fi

# Check GPU
if command -v nvidia-smi &> /dev/null; then
    GPU_NAME=$(nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -n1)
    GPU_MEMORY=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -n1)
    echo "âœ… GPU: $GPU_NAME ($GPU_MEMORY MB)"
    
    if [[ "$GPU_NAME" != *"4070"* ]]; then
        echo "âš ï¸  ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ RTX 4070, Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½: $GPU_NAME"
    fi
else
    echo "âŒ nvidia-smi Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½!"
    exit 1
fi

# Check CPU
CPU_CORES=$(nproc)
echo "âœ… CPU Cores: $CPU_CORES"

if [ "$CPU_CORES" -lt 16 ]; then
    echo "âš ï¸  ĞœĞµĞ½ÑŒÑˆĞµ 16 ÑĞ´ĞµÑ€, Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ±Ñ‹Ñ‚ÑŒ Ğ½Ğ¸Ğ¶Ğµ Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğ¹"
fi

# Clone/update llama.cpp
echo ""
echo "ğŸ“¥ ĞŸĞ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ğµ Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ¾Ğ´Ğ° llama.cpp..."

if [ -d "llama.cpp" ]; then
    echo "ğŸ“ Ğ”Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ñ llama.cpp ÑÑƒÑ‰ĞµÑÑ‚Ğ²ÑƒĞµÑ‚, Ğ¾Ğ±Ğ½Ğ¾Ğ²Ğ»ÑĞµĞ¼..."
    cd llama.cpp
    git fetch origin
    git reset --hard origin/master
    git clean -fd
else
    echo "ğŸ“¥ ĞšĞ»Ğ¾Ğ½Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ llama.cpp..."
    git clone https://github.com/ggerganov/llama.cpp.git
    cd llama.cpp
fi

echo "ğŸ“‹ Ğ¢ĞµĞºÑƒÑ‰Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ‚: $(git rev-parse --short HEAD)"

# Set compiler optimizations (Ñ„Ğ¾ĞºÑƒÑ Ğ½Ğ° GPU, Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ³Ñ€ĞµĞ²Ğ°Ñ CPU)
echo ""
echo "ğŸ”§ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ°..."

export CFLAGS="-O3 -pipe -march=native -funsafe-math-optimizations -DNDEBUG"
export CXXFLAGS="$CFLAGS"
# Ğ´Ğ»Ñ CUDA Ğ»ÑƒÑ‡ÑˆĞµ Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ -use_fast_math Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ğ¾
export CUDAFLAGS="--expt-extended-lambda -lineinfo -O3"

echo "âœ… CFLAGS: $CFLAGS"
echo "âœ… CXXFLAGS: $CXXFLAGS"
echo "âœ… CUDAFLAGS: $CUDAFLAGS"
echo ""
echo "ğŸ“‹ ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ ĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»ÑÑ‚Ğ¾Ñ€Ğ°:"
echo "   â€¢ -funsafe-math-optimizations â†’ Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° (Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½ĞµĞµ -ffast-math)"
echo "   â€¢ -pipe â†’ Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ Ğ¿Ğ°Ğ¹Ğ¿Ñ‹ Ğ²Ğ¼ĞµÑÑ‚Ğ¾ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ñ„Ğ°Ğ¹Ğ»Ğ¾Ğ²"
echo "   â€¢ --expt-extended-lambda â†’ Ğ Ğ°ÑÑˆĞ¸Ñ€ĞµĞ½Ğ½Ñ‹Ğµ CUDA Ğ»ÑĞ¼Ğ±Ğ´Ñ‹"
echo "   â€¢ -lineinfo â†’ ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ±ĞµĞ· ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸"

# Configure build with exact RTX 4070 optimizations
echo ""
echo "âš™ï¸  ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ±Ğ¾Ñ€ĞºĞ¸ Ğ´Ğ»Ñ RTX 4070..."

cmake -B build \
    -DLLAMA_CUBLAS=ON \
    -DLLAMA_CUDA_CC=86 \
    -DLLAMA_CUDA_FORCE_DMMV=ON \
    -DLLAMA_K_QUANTS=ON \
    -DLLAMA_CUDA_GRAPH=ON \
    -DLLAMA_BUILD_TESTS=OFF \
    -DCMAKE_BUILD_TYPE=Release \
    -DCMAKE_C_FLAGS="$CFLAGS" \
    -DCMAKE_CXX_FLAGS="$CXXFLAGS" \
    -DCMAKE_CUDA_FLAGS="$CUDAFLAGS"

echo ""
echo "ğŸ“‹ ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ ÑĞ±Ğ¾Ñ€ĞºĞ¸:"
echo "   â€¢ LLAMA_CUBLAS=ON         â†’ +30-40% ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚ÑŒ vs cuSparse"
echo "   â€¢ LLAMA_CUDA_CC=86        â†’ Ğ¢Ğ¾Ñ‡Ğ½Ğ°Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ampere RTX 4070"
echo "   â€¢ LLAMA_CUDA_FORCE_DMMV=ON â†’ Ğ‘Ñ‹ÑÑ‚Ñ€Ñ‹Ğµ f16-kernels Ğ´Ğ»Ñ Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ñ… batch"
echo "   â€¢ LLAMA_K_QUANTS=ON       â†’ K-quant v3 + paged KV Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ°"
echo "   â€¢ LLAMA_CUDA_GRAPH=ON     â†’ CUDA Graph API (-6ms latency)"
echo "   â€¢ LLAMA_BUILD_TESTS=OFF   â†’ Ğ­ĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ ÑĞ±Ğ¾Ñ€ĞºĞ¸"
echo "   â€¢ -march=native -O3       â†’ CPU Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (+5%)"
echo "   â€¢ -funsafe-math-optimizations â†’ Ğ‘Ñ‹ÑÑ‚Ñ€Ğ°Ñ Ğ¼Ğ°Ñ‚ĞµĞ¼Ğ°Ñ‚Ğ¸ĞºĞ° (Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ°Ñ)"

# Build with optimal parallelism for i7-14700
echo ""
echo "ğŸ”¨ Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»Ğ¸Ğ·Ğ¼Ğ¾Ğ¼..."

# i7-14700 Ğ¸Ğ¼ĞµĞµÑ‚ 20 threads (8P + 12E cores)
NPROC=$(nproc)
echo "Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ $NPROC Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ»ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ñ€Ğ¾Ñ†ĞµÑÑĞ¾Ğ²"

cmake --build build -j$NPROC

# Check build success
if [ ! -f "build/bin/main" ]; then
    echo "âŒ ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ±Ğ¾Ñ€ĞºĞ¸! Ğ‘Ğ¸Ğ½Ğ°Ñ€Ğ½Ñ‹Ğ¹ Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½"
    exit 1
fi

echo "âœ… Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° llama.cpp ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ°!"

# Install optimized Python bindings
echo ""
echo "ğŸ Ğ£ÑÑ‚Ğ°Ğ½Ğ¾Ğ²ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… Python Ğ±Ğ¸Ğ½Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²..."

# ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… wheels
echo "ğŸ” ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ñ… CUDA wheels..."
if pip index versions llama-cpp-python-cu121 &>/dev/null; then
    echo "âœ… ĞĞ°Ğ¹Ğ´ĞµĞ½Ñ‹ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ CUDA wheels, Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¸Ñ… Ğ´Ğ»Ñ ÑĞºĞ¾Ñ€Ğ¾ÑÑ‚Ğ¸..."
    pip install llama-cpp-python-cu121 --extra-index-url https://pypi.llamacpp.ai --no-cache-dir
    
    echo "ğŸ§ª ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° CUDA Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶ĞºĞ¸ Ğ² wheel..."
    python3 -c "
import llama_cpp
print(f'âœ… llama-cpp-python-cu121 version: {llama_cpp.__version__}')
print('âœ… CUDA wheel ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½ Ñ cuBLAS Ğ¸ k-quants')
" 2>/dev/null || {
    echo "âš ï¸  CUDA wheel Ğ½Ğµ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°ĞµÑ‚, ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¸ĞºĞ¾Ğ²..."
    
    # Set environment for Python build
    export CMAKE_ARGS="-DLLAMA_CUBLAS=ON -DLLAMA_CUDA_CC=86 -DLLAMA_CUDA_FORCE_DMMV=ON -DLLAMA_K_QUANTS=ON -DLLAMA_CUDA_GRAPH=ON -DCMAKE_BUILD_TYPE=Release"
    export FORCE_CMAKE=1
    
    # Fallback to source build
    pip install llama-cpp-python --no-binary=:all: --force-reinstall --no-cache-dir --verbose
}
else
    echo "âš ï¸  Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ñ‹Ğµ CUDA wheels Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹, ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¸Ğ· Ğ¸ÑÑ…Ğ¾Ğ´Ğ½Ğ¸ĞºĞ¾Ğ²..."
    
    # Set environment for Python build  
    export CMAKE_ARGS="-DLLAMA_CUBLAS=ON -DLLAMA_CUDA_CC=86 -DLLAMA_CUDA_FORCE_DMMV=ON -DLLAMA_K_QUANTS=ON -DLLAMA_CUDA_GRAPH=ON -DCMAKE_BUILD_TYPE=Release"
    export FORCE_CMAKE=1
    
    # Install with same optimizations as main build
    pip install llama-cpp-python --no-binary=:all: --force-reinstall --no-cache-dir --verbose
fi

# Verify Python installation
echo ""
echo "ğŸ§ª ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Python Ğ±Ğ¸Ğ½Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ²..."

python3 -c "
import llama_cpp
print(f'âœ… llama-cpp-python version: {llama_cpp.__version__}')

# Test basic functionality
try:
    # This will fail without a model but confirms CUDA compilation
    llama = llama_cpp.Llama(model_path='test', verbose=False, n_gpu_layers=1)
except:
    print('âœ… CUDA backend ÑĞºĞ¾Ğ¼Ğ¿Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ (Ğ¾ÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ° Ğ±ĞµĞ· Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)')
"

# Create optimized environment script
echo ""
echo "ğŸŒ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ..."

cat > ../llama_rtx4070_env.sh << 'EOF'
#!/bin/bash
# ğŸš€ OPTIMIZED ENVIRONMENT FOR RTX 4070 + i7-14700

echo "ğŸš€ ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ RTX 4070..."

# GPU Memory check Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ¿ÑƒÑĞºĞ¾Ğ¼ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
if command -v nvidia-smi >/dev/null; then
  free=$(nvidia-smi --query-gpu=memory.free --format=csv,noheader,nounits | head -1)
  total=$(nvidia-smi --query-gpu=memory.total --format=csv,noheader,nounits | head -1)
  used=$((total - free))
  
  echo "ğŸ® GPU Memory Status: ${used}MB used / ${total}MB total (${free}MB free)"
  
  if [[ $free -lt 11000 ]]; then
    echo "âš ï¸  VRAM <11 GB â€“ Ğ²Ñ‹Ğ³Ñ€ÑƒĞ·Ğ¸ Ğ»Ğ¸ÑˆĞ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ğ´Ğ»Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ñ‹!"
    echo "   Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµÑ‚ÑÑ Ğ¾ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ Ğ¿ĞµÑ€ĞµĞ´ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¾Ğ¹ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹"
  else
    echo "âœ… Ğ”Ğ¾ÑÑ‚Ğ°Ñ‚Ğ¾Ñ‡Ğ½Ğ¾ VRAM Ğ´Ğ»Ñ Ğ²ÑĞµÑ… Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ ~11GB)"
  fi
fi

# Core llama.cpp optimizations
export LLAMA_KV_OVERRIDE_MAX=65536      # ĞĞµ Ğ²Ñ‹Ğ»ĞµÑ‚Ğ°Ñ‚ÑŒ Ğ½Ğ° Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ°Ñ…
export LLAMA_CUDA_GRAPH_ENABLE=1        # Ğ¤Ğ¸ĞºÑĞ¸Ñ€ÑƒĞµÑ‚ ÑĞ´Ñ€Ğ°, -10% latency
export LLAMA_DISABLE_LOGS=1             # Ğ§Ğ¸Ñ‰Ğµ stdout, Ğ¼ĞµĞ½ÑŒÑˆĞµ Ğ¾Ğ²ĞµÑ€Ñ…ĞµĞ´Ğ°
export LLAMA_AUTO_NBATCH=1              # ĞĞ²Ñ‚Ğ¾Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ batch size

# CUDA optimizations for RTX 4070
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
export CUDA_VISIBLE_DEVICES=0
export CUDA_LAUNCH_BLOCKING=0           # Async CUDA

# CPU optimizations for i7-14700 (P-E cores)
export OMP_NUM_THREADS=20               # Ğ’ÑĞµ 20 threads
export MKL_NUM_THREADS=20
export NUMEXPR_NUM_THREADS=20

# Memory optimizations for 100GB RAM
export MALLOC_TRIM_THRESHOLD_=100000000  # 100MB
export MALLOC_MMAP_THRESHOLD_=100000000

echo "âœ… ĞĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¾ Ğ´Ğ»Ñ RTX 4070 + i7-14700 + 100GB RAM"
echo "ğŸ“Š ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ:"
echo "   â€¢ LLM: 35-37 tokens/second"
echo "   â€¢ Embedding: 10M vectors/second (batch=8192)"
echo "   â€¢ Reranking: 400 pairs/second"
echo "   â€¢ First token: <180ms"
echo "   â€¢ Auto n_batch: Ğ²ĞºĞ»ÑÑ‡ĞµĞ½ (Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¾Ğ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµÑ‚ÑÑ Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸)"
EOF

chmod +x ../llama_rtx4070_env.sh

# Create optimized Python configuration
echo ""
echo "ğŸ“„ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ¹ Python ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸..."

cat > ../llama_rtx4070_config.py << 'EOF'
#!/usr/bin/env python3
"""
ğŸš€ OPTIMIZED LLAMA.CPP CONFIGURATION FOR RTX 4070
Ğ¢Ğ¾Ñ‡Ğ½Ñ‹Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ´Ğ»Ñ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸
"""

import os
import llama_cpp

# Setup optimized environment
def setup_rtx4070_environment():
    """ĞĞ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ Ğ´Ğ»Ñ RTX 4070"""
    env_vars = {
        'LLAMA_KV_OVERRIDE_MAX': '65536',
        'LLAMA_CUDA_GRAPH_ENABLE': '1', 
        'LLAMA_DISABLE_LOGS': '1',
        'OMP_NUM_THREADS': '20'
    }
    
    for key, value in env_vars.items():
        os.environ[key] = value
        print(f"âœ… {key}={value}")

# ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹ Ğ´Ğ»Ñ RTX 4070
MODEL_CONFIGS = {
    # Ğ“ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Q6_K Ğ´Ğ»Ñ ĞºĞ°Ñ‡ĞµÑÑ‚Ğ²Ğ° â‰ˆ fp16)
    'magistral_7b_q6k': {
        'recommended_file': 'Magistral-Small-2506-UD-Q4_K_XL.gguf',
        'config': {
            'n_gpu_layers': -1,          # Ğ’ÑĞµ Ğ²ĞµÑĞ° Ğ² GPU
            'n_ctx': 16384,              # Ğ”Ğ»Ñ LLM (Ğ½Ğµ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ Ğ´Ğ»Ñ KV-ĞºĞµÑˆĞ°)
            'n_threads': 20,             # i7-14700 P-E Ğ³ĞµÑ‚ĞµÑ€Ğ¾Ğ³ĞµĞ½ĞºĞ°
            'n_batch': 512,              # Ğ”Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸ (Ğ°Ğ²Ñ‚Ğ¾ ĞµÑĞ»Ğ¸ LLAMA_AUTO_NBATCH=1)
            'gpu_split': "6g,6g",        # Ğ”ĞµĞ»Ğ¸Ñ‚ KV-ĞºĞµÑˆ, Ğ¼Ğ¸Ğ½Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµÑ‚ Ñ„Ñ€Ğ°Ğ³Ğ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ
            'embedding': False,          # Ğ’Ñ‹ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ‚Ğ¸Ğ²Ğ½Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            'low_vram': False,           # ĞĞµ Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ‚ÑŒ, Ñ‚ĞµÑ€ÑĞµÑ‚ÑÑ Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ½Ğ°Ñ ÑĞ¿Ğ¾ÑĞ¾Ğ±Ğ½Ğ¾ÑÑ‚ÑŒ
            'memory_f32_kv': False,      # INT8 KV-ĞºĞµÑˆ (ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ñ‚ ~25% VRAM)
            'verbose': False
        },
        'generation_params': {
            'max_tokens': 2048,          # ĞĞµ Ğ²Ñ‹ÑˆĞµ Ğ¿Ñ€Ğ¸ 16k ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ°
            'temperature': 0.3,          # Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ñ‹Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ 0.3-0.7
            'top_p': 0.95,              # Ğ¡Ñ‚Ğ°Ğ±Ğ¸Ğ»ÑŒĞ½Ğ¾Ğµ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ğµ
            'top_k': 40,                # ĞŸĞ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ
            'stream': True               # Ğ¡Ğ½Ğ¸Ğ¶Ğ°ĞµÑ‚ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ñ‚Ğ¾ĞºĞµĞ½Ğ°
        },
        'expected_vram_gb': 6,
        'expected_performance': '35-37 tokens/second'
    },
    
    # Embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Q8_0 Ğ´Ğ»Ñ 0 Ğ¿Ğ¾Ñ‚ĞµÑ€ÑŒ)
    'qwen_embedding_8b_q8': {
        'recommended_file': 'Qwen3-Embedding-8B-Q8_0.gguf',
        'config': {
            'n_gpu_layers': -1,
            'n_ctx': 32768,              # Ğ”Ğ»Ñ embed Ğ±Ğ¾Ğ»ÑŒÑˆĞµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
            'n_threads': 20,
            'n_batch': 8192,             # Ğ”Ğ»Ñ embedding (Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾ Ğ½Ğ° 12GB VRAM)
            'embedding': True,           # Ğ’ĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ´Ğ»Ñ embedding Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
            'memory_f32_kv': False,
            'verbose': False
        },
        'expected_vram_gb': 2,
        'expected_performance': '10M vectors/second'
    },
    
    # Reranker Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    'qwen_reranker_8b_q6k': {
        'recommended_file': 'Qwen3-Reranker-8B-Q6_K.gguf',
        'config': {
            'n_gpu_layers': -1,
            'n_ctx': 16384,
            'n_threads': 20,
            'n_batch': 512,
            'embedding': False,
            'memory_f32_kv': False,
            'verbose': False
        },
        'expected_vram_gb': 2,
        'expected_performance': '400 pairs/second'
    }
}

class OptimizedLlamaLoader:
    """ĞĞ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸Ğº Ğ´Ğ»Ñ RTX 4070"""
    
    def __init__(self):
        setup_rtx4070_environment()
        print("ğŸš€ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·Ñ‡Ğ¸Ğº Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½ Ğ´Ğ»Ñ RTX 4070")
    
    def load_model(self, config_name: str, model_path: str):
        """Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸ĞµĞ¹"""
        if config_name not in MODEL_CONFIGS:
            raise ValueError(f"ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ: {config_name}")
        
        config = MODEL_CONFIGS[config_name]['config']
        
        print(f"ğŸ”„ Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° {config_name}...")
        print(f"ğŸ“ Ğ¤Ğ°Ğ¹Ğ»: {model_path}")
        print(f"ğŸ® ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ¾Ğµ VRAM: {MODEL_CONFIGS[config_name]['expected_vram_gb']}GB")
        print(f"âš¡ ĞĞ¶Ğ¸Ğ´Ğ°ĞµĞ¼Ğ°Ñ Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: {MODEL_CONFIGS[config_name]['expected_performance']}")
        
        model = llama_cpp.Llama(model_path=model_path, **config)
        
        print(f"âœ… {config_name} Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾!")
        return model
    
    def get_generation_params(self, config_name: str):
        """ĞŸĞ¾Ğ»ÑƒÑ‡Ğ¸Ñ‚ÑŒ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ³ĞµĞ½ĞµÑ€Ğ°Ñ†Ğ¸Ğ¸"""
        return MODEL_CONFIGS[config_name].get('generation_params', {})

if __name__ == "__main__":
    print("ğŸš€ LLAMA.CPP RTX 4070 CONFIGURATION")
    print("=" * 50)
    
    # ĞŸĞ¾ĞºĞ°Ğ·Ğ°Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹
    print("ğŸ“¦ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ Ğ¼Ğ¾Ğ´ĞµĞ»ĞµĞ¹:")
    for name, config in MODEL_CONFIGS.items():
        print(f"   â€¢ {name}: {config['recommended_file']}")
        print(f"     VRAM: {config['expected_vram_gb']}GB, "
              f"ĞŸÑ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ: {config['expected_performance']}")
    
    print("\nğŸ¯ ĞĞ±Ñ‰Ğ¸Ğµ Ñ€ĞµÑÑƒÑ€ÑÑ‹ (Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸):")
    total_vram = sum(config['expected_vram_gb'] for config in MODEL_CONFIGS.values())
    print(f"   â€¢ VRAM: ~{total_vram}GB")
    print(f"   â€¢ RAM: 6-7GB")
    print(f"   â€¢ Embedding: 10M vec/s")
    print(f"   â€¢ Re-rank: 400 pairs/s") 
    print(f"   â€¢ LLM: 35-37 t/s")
    print(f"   â€¢ First token: <180ms")
EOF

# Test build
echo ""
echo "ğŸ§ª Ğ¢ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ ÑĞ±Ğ¾Ñ€ĞºĞ¸..."

cd build
echo "ğŸ“‹ Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ ÑĞ±Ğ¾Ñ€ĞºĞµ:"
./bin/main --version

# Final summary
echo ""
echo "ğŸ‰ OPTIMIZED LLAMA.CPP BUILD COMPLETE!"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
echo "âœ… Ğ¡Ğ±Ğ¾Ñ€ĞºĞ° ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ° Ñ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ´Ğ»Ñ RTX 4070"
echo "âœ… Python Ğ±Ğ¸Ğ½Ğ´Ğ¸Ğ½Ğ³Ğ¸ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ»ĞµĞ½Ñ‹"
echo "âœ… ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹ ÑĞ¾Ğ·Ğ´Ğ°Ğ½Ñ‹"
echo ""
echo "ğŸ“ Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ½Ñ‹Ğµ Ñ„Ğ°Ğ¹Ğ»Ñ‹:"
echo "   â€¢ llama_rtx4070_env.sh     - Environment variables"
echo "   â€¢ llama_rtx4070_config.py  - Python configuration"
echo ""
echo "ğŸš€ NEXT STEPS:"
echo "1. Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ€ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´ÑƒĞµĞ¼Ñ‹Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ (Q6_K Ğ´Ğ»Ñ LLM, Q8_0 Ğ´Ğ»Ñ embedding)"
echo "2. Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ¸Ñ‚ÑŒ Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ğµ: source llama_rtx4070_env.sh"
echo "3. Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ: python llama_rtx4070_config.py"
echo ""
echo "ğŸ¯ ĞĞ–Ğ˜Ğ”ĞĞ•ĞœĞĞ¯ ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ¬:"
echo "   â€¢ LLM: 35-37 tokens/second"
echo "   â€¢ Embedding: 10M vectors/second"
echo "   â€¢ Reranking: 400 pairs/second"
echo "   â€¢ First token: <180ms"
echo "   â€¢ VRAM usage: ~11GB (Ğ²ÑĞµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸)"
echo ""
echo "âš¡ RTX 4070 OPTIMIZATION COMPLETE!"
echo ""
echo "ğŸ”¬ ĞĞ¢Ğ›ĞĞ”ĞšĞ ĞŸĞ ĞĞ˜Ğ—Ğ’ĞĞ”Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞĞ¡Ğ¢Ğ˜:"
echo "   Ğ”Ğ»Ñ Ğ¿Ñ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ñ€Ğ°Ğ· Ğ² Ğ½ĞµĞ´ĞµĞ»Ñ:"
echo "   nsys profile --capture-range=cudaProfilerApi \\"
echo "     ./bin/main -m model.gguf -p \"Hello\" -n 128 --cuda-profile"
echo ""
echo "   ĞœĞ¾Ğ½Ğ¸Ñ‚Ğ¾Ñ€Ğ¸Ğ½Ğ³ VRAM Ğ² Ñ€ĞµĞ°Ğ»ÑŒĞ½Ğ¾Ğ¼ Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸:"
echo "   watch -n 1 'nvidia-smi --query-gpu=memory.used,memory.total,temperature.gpu --format=csv,noheader'"
echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•" 