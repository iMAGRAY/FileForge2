#!/usr/bin/env python3
"""
üöÄ EMBEDDING SERVER –¥–ª—è RTX 4070
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏ LLM –æ—Ç–≤–µ—Ç–æ–≤
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç llama.cpp –Ω–∞–ø—Ä—è–º—É—é —Å OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º API –¥–ª—è Cursor
"""

import asyncio
import json
import logging
import os
import sys
import time
import subprocess
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple, Union
import hashlib
import pickle
from datetime import datetime, timedelta

# –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
import numpy as np
import faiss
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer

# FastAPI –¥–ª—è HTTP —Å–µ—Ä–≤–µ—Ä–∞
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# FlashRank –¥–ª—è —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    from flashrank import Ranker, RerankRequest
    FLASHRANK_AVAILABLE = True
except ImportError:
    print("‚ùå flashrank –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install flashrank")
    FLASHRANK_AVAILABLE = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Cursor Compatible Embedding Server",
    description="–õ–æ–∫–∞–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å OpenAI API —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å—é",
    version="1.0.0"
)

# CORS –¥–ª—è –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è —Å–µ—Ä–≤–µ—Ä–∞
embedding_server = None

# === OpenAI API Models ===
class EmbeddingRequest(BaseModel):
    input: Union[List[str], str]
    model: str = "text-embedding-ada-002"
    encoding_format: str = "float"
    dimensions: Optional[int] = None

class EmbeddingData(BaseModel):
    object: str = "embedding"
    embedding: List[float]
    index: int

class EmbeddingUsage(BaseModel):
    prompt_tokens: int
    total_tokens: int

class EmbeddingResponse(BaseModel):
    object: str = "list"
    data: List[EmbeddingData]
    model: str
    usage: EmbeddingUsage

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "local"

class ModelsResponse(BaseModel):
    object: str = "list"
    data: List[ModelInfo]

class LlamaCppInterface:
    """–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å llama.cpp"""
    
    def __init__(self, executable_path: str, model_path: str, gpu_layers: int = 35):
        self.executable_path = executable_path
        self.model_path = model_path
        self.gpu_layers = gpu_layers
        self.process = None
        
    async def start_server(self, port: int = 8080, embedding_mode: bool = False):
        """–ó–∞–ø—É—Å–∫ llama.cpp —Å–µ—Ä–≤–µ—Ä–∞"""
        cmd = [
            self.executable_path,
            "-m", self.model_path,
            "--port", str(port),
            "--host", "127.0.0.1",
            "-ngl", str(self.gpu_layers),
            "-c", "2048",
            "--log-disable"
        ]
        
        if embedding_mode:
            cmd.append("--embedding")
        
        logger.info(f"üöÄ –ó–∞–ø—É—Å–∫ llama.cpp —Å–µ—Ä–≤–µ—Ä–∞: {' '.join(cmd)}")
        
        self.process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞
        await asyncio.sleep(3)
        
        if self.process.returncode is not None:
            stdout, stderr = await self.process.communicate()
            raise RuntimeError(f"–û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ llama.cpp: {stderr.decode()}")
        
        logger.info(f"‚úÖ llama.cpp —Å–µ—Ä–≤–µ—Ä –∑–∞–ø—É—â–µ–Ω –Ω–∞ –ø–æ—Ä—Ç—É {port}")
        
    async def stop_server(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Ä–≤–µ—Ä–∞"""
        if self.process:
            self.process.terminate()
            await self.process.wait()
            self.process = None
            
    async def generate_embedding(self, text: str, port: int = 8080) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ HTTP API"""
        import aiohttp
        
        url = f"http://127.0.0.1:{port}/embedding"
        payload = {"content": text}
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    result = await response.json()
                    # llama.cpp –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å –æ–±—ä–µ–∫—Ç–∞–º–∏
                    if isinstance(result, list) and len(result) > 0:
                        embedding_data = result[0]["embedding"]
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å–ª–∏ embedding —ç—Ç–æ –≤–ª–æ–∂–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫
                        if isinstance(embedding_data, list) and len(embedding_data) > 0 and isinstance(embedding_data[0], list):
                            embedding_data = embedding_data[0]
                        return np.array(embedding_data, dtype=np.float32)
                    else:
                        raise RuntimeError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {result}")
                else:
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ API llama.cpp: {response.status}")
    
    async def generate_completion(self, prompt: str, port: int = 8081, **kwargs) -> Dict[str, Any]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ —á–µ—Ä–µ–∑ HTTP API"""
        import aiohttp
        
        url = f"http://127.0.0.1:{port}/completion"
        payload = {
            "prompt": prompt,
            "n_predict": kwargs.get("max_tokens", 512),
            "temperature": kwargs.get("temperature", 0.7),
            "top_p": kwargs.get("top_p", 0.9),
            "repeat_penalty": kwargs.get("repeat_penalty", 1.1),
            "stop": kwargs.get("stop", ["</s>", "<|im_end|>"])
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(url, json=payload) as response:
                if response.status == 200:
                    return await response.json()
                else:
                    raise RuntimeError(f"–û—à–∏–±–∫–∞ API llama.cpp: {response.status}")

class EmbeddingServer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–µ—Ä–≤–µ—Ä–∞"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.indexes = {}
        self.cache = {}
        self.llama_processes = {}
        
        # –ü—É—Ç–∏ –∫ –∏—Å–ø–æ–ª–Ω—è–µ–º—ã–º —Ñ–∞–π–ª–∞–º –∏ –º–æ–¥–µ–ª—è–º
        self.llama_cpp_path = config.get('llama_cpp_path', './llama_build/llama.cpp/build/bin/Release/llama-server.exe')
        self.model_paths = {
            'embedding': config.get('embedding_model_path', './models/Qwen3-Embedding-8B-Q6_K.gguf'),
            'reranker': config.get('reranker_model_path', './models/Qwen3-Reranker-8B-Q6_K.gguf'),
            'coder': config.get('coder_model_path', './models/Qwen2.5-Coder-7B-Instruct.Q6_K.gguf'),
            'llm': config.get('llm_model_path', './models/Magistral-Small-2506-UD-Q4_K_XL.gguf')
        }
        
        # –ü–æ—Ä—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
        self.ports = {
            'embedding': 8080,
            'coder': 8081,
            'llm': 8082
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPU
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.gpu_layers = config.get('gpu_layers', 35)  # –î–ª—è RTX 4070
        
        # –ö—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.cache_dir = Path(config.get('cache_dir', './cache'))
        self.cache_dir.mkdir(exist_ok=True)

        # –ü—É—Ç—å –∫ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—é –¥–ª—è –∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–∏—è
        self.repo_path = Path(config.get('repo_path', '.')).resolve()
        self.watcher = None
        
        logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Embedding Server –¥–ª—è {self.device}")
        logger.info(f"üíæ –ö—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.cache_dir}")
        logger.info(f"üîß llama.cpp –ø—É—Ç—å: {self.llama_cpp_path}")

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è llama.cpp
        if not os.path.exists(self.llama_cpp_path):
            logger.error(f"‚ùå llama.cpp –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.llama_cpp_path}")
            logger.info("üí° –ü–æ—Å—Ç—Ä–æ–π—Ç–µ llama.cpp: cd llama_build && make")
            return
        
        # –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ llama.cpp
        await self._start_llama_servers()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        await self._load_reranker_model()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
        await self._initialize_indexes()

        # –ü–µ—Ä–≤–∏—á–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è
        await self.index_repository(self.repo_path)

        # –ó–∞–ø—É—Å–∫ –Ω–∞–±–ª—é–¥–∞—Ç–µ–ª—è –∑–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–º
        if self.repo_path.exists():
            from .repo_watcher import RepositoryWatcher
            self.watcher = RepositoryWatcher(self, self.repo_path)
            self.watcher.start()
        else:
            logger.warning(f"‚ö†Ô∏è –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {self.repo_path}")

        logger.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        self._print_memory_usage()

    async def _start_llama_servers(self):
        """–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–µ—Ä–æ–≤ llama.cpp –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        # Embedding —Å–µ—Ä–≤–µ—Ä
        if os.path.exists(self.model_paths['embedding']):
            logger.info(f"üì• –ó–∞–ø—É—Å–∫ Qwen3-Embedding-8B —Å–µ—Ä–≤–µ—Ä–∞")
            self.llama_processes['embedding'] = LlamaCppInterface(
                self.llama_cpp_path,
                self.model_paths['embedding'],
                self.gpu_layers
            )
            await self.llama_processes['embedding'].start_server(
                self.ports['embedding'], 
                embedding_mode=True
            )
        else:
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_paths['embedding']}")
        
        # Coder —Å–µ—Ä–≤–µ—Ä (Qwen2.5)
        if os.path.exists(self.model_paths['coder']):
            logger.info(f"üì• –ó–∞–ø—É—Å–∫ Qwen2.5-Coder-7B —Å–µ—Ä–≤–µ—Ä–∞")
            self.llama_processes['coder'] = LlamaCppInterface(
                self.llama_cpp_path,
                self.model_paths['coder'],
                self.gpu_layers
            )
            await self.llama_processes['coder'].start_server(self.ports['coder'])
        else:
            logger.warning(f"‚ö†Ô∏è Coder –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_paths['coder']}")

        # LLM —Å–µ—Ä–≤–µ—Ä (Magistral)
        if os.path.exists(self.model_paths['llm']):
            logger.info(f"üì• –ó–∞–ø—É—Å–∫ Magistral-Small-2506 —Å–µ—Ä–≤–µ—Ä–∞")
            self.llama_processes['llm'] = LlamaCppInterface(
                self.llama_cpp_path,
                self.model_paths['llm'],
                self.gpu_layers
            )
            await self.llama_processes['llm'].start_server(self.ports['llm'])
        else:
            logger.warning(f"‚ö†Ô∏è LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {self.model_paths['llm']}")

    async def _load_reranker_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        if not FLASHRANK_AVAILABLE:
            logger.warning("‚ö†Ô∏è FlashRank –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            return
            
        try:
            logger.info("üì• –ó–∞–≥—Ä—É–∑–∫–∞ FlashRank —Ä–µ-—Ä–∞–Ω–∫–µ—Ä–∞...")
            self.models['reranker'] = Ranker(model_name="ms-marco-MiniLM-L-12-v2", cache_dir=str(self.cache_dir))
            logger.info("‚úÖ FlashRank —Ä–µ-—Ä–∞–Ω–∫–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ-—Ä–∞–Ω–∫–µ—Ä–∞: {e}")

    async def _initialize_indexes(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞"""
        logger.info("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS –∏–Ω–¥–µ–∫—Å–∞...")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ FAISS –∏–Ω–¥–µ–∫—Å–∞
        embedding_dim = 4096  # Qwen3-Embedding-8B —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
        self.indexes['faiss'] = faiss.IndexHNSWFlat(embedding_dim, 32)
        self.indexes['faiss'].hnsw.efConstruction = 40
        self.indexes['faiss'].hnsw.efSearch = 16
        
        # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        self.indexes['metadata'] = []

        logger.info("‚úÖ FAISS –∏–Ω–¥–µ–∫—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    async def index_repository(self, repo_path: Path):
        """–ü–æ–ª–Ω–∞—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è."""
        if not repo_path.exists():
            logger.warning(f"‚ö†Ô∏è –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {repo_path}")
            return

        logger.info(f"üìÇ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è {repo_path}...")

        for file_path in repo_path.rglob('*'):
            if not file_path.is_file():
                continue

            try:
                from .chunker import chunk_file
                chunks = chunk_file(str(file_path))
                if not chunks:
                    continue
                rel = os.path.relpath(file_path, repo_path)
                meta = [{"path": rel, "chunk_id": i} for i in range(len(chunks))]
                await self.add_to_index(chunks, meta)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ {file_path}: {e}")

        logger.info("‚úÖ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω")

    def _print_memory_usage(self):
        """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
        if torch.cuda.is_available():
            total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            allocated_memory = torch.cuda.memory_allocated(0) / 1024**3
            logger.info(f"üîß GPU –ø–∞–º—è—Ç—å: {allocated_memory:.1f}/{total_memory:.1f} GB")
        else:
            logger.info("üîß GPU –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")

    async def generate_embedding(self, text: str, use_cache: bool = True) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            Numpy –º–∞—Å—Å–∏–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
        """
        if 'embedding' not in self.llama_processes:
            raise ValueError("Embedding —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        if use_cache:
            text_hash = hashlib.md5(text.encode()).hexdigest()
            cache_path = self.cache_dir / f"embedding_{text_hash}.pkl"
            
            if cache_path.exists():
                try:
                    with open(cache_path, 'rb') as f:
                        cached_embedding = pickle.load(f)
                    logger.debug(f"üíæ –ó–∞–≥—Ä—É–∂–µ–Ω —ç–º–±–µ–¥–¥–∏–Ω–≥ –∏–∑ –∫—ç—à–∞")
                    return cached_embedding
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
        
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ —á–µ—Ä–µ–∑ llama.cpp
            embedding = await self.llama_processes['embedding'].generate_embedding(
                text, 
                port=self.ports['embedding']
            )
            
            generation_time = time.time() - start_time
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            if use_cache:
                try:
                    with open(cache_path, 'wb') as f:
                        pickle.dump(embedding, f)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –∫—ç—à: {e}")
            
            logger.info(f"‚ö° –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {generation_time:.3f}s, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {embedding.shape}")
            
            return embedding
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            raise

    async def rerank_documents(
        self, 
        query: str, 
        documents: List[str], 
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        """
        if 'reranker' not in self.models:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            return [
                {
                    'text': doc,
                    'score': 1.0 - (i * 0.1),  # –£–±—ã–≤–∞—é—â–∏–µ –æ—Ü–µ–Ω–∫–∏
                    'rank': i + 1,
                    'corpus_id': i
                }
                for i, doc in enumerate(documents[:top_k])
            ]
        
        try:
            # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è FlashRank
            rerank_request = RerankRequest(query=query, passages=documents)
            
            # –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            results = self.models['reranker'].rerank(rerank_request)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            ranked_docs = []
            for i, result in enumerate(results[:top_k]):
                ranked_docs.append({
                    'text': documents[result['corpus_id']],
                    'score': result['score'],
                    'rank': i + 1,
                    'corpus_id': result['corpus_id']
                })
            
            logger.info(f"üìä –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {len(ranked_docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
            
            return ranked_docs
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
            return [
                {
                    'text': doc,
                    'score': 1.0 - (i * 0.1),
                    'rank': i + 1,
                    'corpus_id': i
                }
                for i, doc in enumerate(documents[:top_k])
            ]

    async def generate_response(
        self, 
        prompt: str, 
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM
        
        Args:
            prompt: –í—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if 'coder' not in self.llama_processes and 'llm' not in self.llama_processes:
            raise ValueError("LLM —Å–µ—Ä–≤–µ—Ä–∞ –Ω–µ –∑–∞–ø—É—â–µ–Ω—ã")
            
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
            results = []
            for name in ['coder', 'llm']:
                if name in self.llama_processes:
                    resp = await self.llama_processes[name].generate_completion(
                        prompt,
                        port=self.ports[name],
                        max_tokens=max_tokens,
                        temperature=temperature
                    )
                    results.append((name, resp.get('content', '').strip()))

            generation_time = time.time() - start_time

            # –ï—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω —Ä–µ-—Ä–∞–Ω–∫–µ—Ä –∏ –µ—Å—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ - –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
            if self.models.get('reranker') and len(results) > 1:
                texts = [r[1] for r in results]
                ranked = await self.rerank_documents(prompt, texts, top_k=1)
                best_index = ranked[0]['corpus_id'] if ranked else 0
            else:
                best_index = 0

            best_name, best_text = results[best_index]

            prompt_tokens = len(prompt.split())
            completion_tokens = len(best_text.split())

            result = {
                'response': best_text,
                'prompt_tokens': prompt_tokens,
                'completion_tokens': completion_tokens,
                'total_tokens': prompt_tokens + completion_tokens,
                'generation_time': generation_time,
                'tokens_per_second': completion_tokens / generation_time if generation_time > 0 else 0,
                'model': best_name
            }

            logger.info(
                f"üí¨ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω {best_name}: {result['tokens_per_second']:.1f} tok/s"
            )

            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            raise

    async def add_to_index(self, texts: List[str], metadata: List[Dict] = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å"""
        if not self.indexes.get('faiss'):
            raise ValueError("FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        logger.info(f"üìù –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = []
        for text in texts:
            embedding = await self.generate_embedding(text)
            embeddings.append(embedding)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ FAISS
        embeddings_matrix = np.vstack(embeddings)
        self.indexes['faiss'].add(embeddings_matrix)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if metadata:
            self.indexes['metadata'].extend(metadata)
        else:
            self.indexes['metadata'].extend([{'text': text} for text in texts])
        
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å")

    async def search_index(
        self, 
        query: str, 
        top_k: int = 10,
        use_rerank: bool = True
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.indexes.get('faiss'):
            raise ValueError("FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = await self.generate_embedding(query)
        query_embedding = query_embedding.reshape(1, -1)
        
        # –ü–æ–∏—Å–∫ –≤ FAISS
        search_k = top_k * 2 if use_rerank else top_k
        scores, indices = self.indexes['faiss'].search(query_embedding, search_k)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.indexes['metadata']):
                doc = self.indexes['metadata'][idx].copy()
                doc['faiss_score'] = float(score)
                doc['faiss_rank'] = i + 1
                documents.append(doc)
        
        # –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if use_rerank and len(documents) > 1 and self.models.get('reranker'):
            texts = [doc.get('text', '') for doc in documents]
            reranked = await self.rerank_documents(query, texts, top_k)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = []
            for rerank_result in reranked:
                original_doc = documents[rerank_result['corpus_id']]
                original_doc.update({
                    'rerank_score': rerank_result['score'],
                    'final_rank': rerank_result['rank']
                })
                final_results.append(original_doc)
            
            return final_results
        
        return documents[:top_k]

    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
        stats = {
            'models_loaded': {
                'embedding': 'embedding' in self.llama_processes,
                'reranker': 'reranker' in self.models,
                'coder': 'coder' in self.llama_processes,
                'llm': 'llm' in self.llama_processes
            },
            'index_size': self.indexes.get('faiss', {}).ntotal if 'faiss' in self.indexes else 0,
            'cache_dir': str(self.cache_dir),
            'device': self.device,
            'model_paths': self.model_paths,
            'llama_cpp_path': self.llama_cpp_path,
            'ports': self.ports
        }
        
        if torch.cuda.is_available():
            stats['gpu_memory'] = {
                'total_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3,
                'allocated_gb': torch.cuda.memory_allocated(0) / 1024**3,
                'reserved_gb': torch.cuda.memory_reserved(0) / 1024**3
            }
        
        return stats

    async def shutdown(self):
        """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã"""
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã Embedding Server...")
        
        # –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –≤—Å–µ—Ö llama.cpp —Å–µ—Ä–≤–µ—Ä–æ–≤
        for name, process in self.llama_processes.items():
            logger.info(f"üõë –û—Å—Ç–∞–Ω–æ–≤–∫–∞ {name} —Å–µ—Ä–≤–µ—Ä–∞...")
            await process.stop_server()

        if self.watcher:
            await self.watcher.stop()

        logger.info("‚úÖ –í—Å–µ —Å–µ—Ä–≤–µ—Ä—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")

# === FASTAPI HTTP –°–ï–†–í–ï–† ===

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–µ—Ä–∞
embedding_server: Optional[EmbeddingServer] = None

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="Cursor Compatible Embedding Server",
    description="–õ–æ–∫–∞–ª—å–Ω—ã–π —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–µ—Ä–≤–µ—Ä —Å OpenAI API –¥–ª—è Cursor",
    version="1.0.0"
)

# CORS –¥–ª—è –≤—Å–µ—Ö –¥–æ–º–µ–Ω–æ–≤
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    global embedding_server
    
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Cursor Compatible Embedding Server...")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–∞
    config = {
        'llama_cpp_path': './llama_build/llama.cpp/build/bin/Release/llama-server.exe',
        'embedding_model_path': './models/Qwen3-Embedding-8B-Q6_K.gguf',
        'reranker_model_path': './models/Qwen3-Reranker-8B-Q6_K.gguf',
        'coder_model_path': './models/Qwen2.5-Coder-7B-Instruct.Q6_K.gguf',
        'llm_model_path': './models/Magistral-Small-2506-UD-Q4_K_XL.gguf',
        'cache_dir': './cache',
        'repo_path': '.',
        'gpu_layers': 35,  # –î–ª—è RTX 4070
    }
    
    embedding_server = EmbeddingServer(config)
    await embedding_server.initialize()
    
    logger.info("‚úÖ Embedding Server –≥–æ—Ç–æ–≤ –∫ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–æ–≤!")

@app.on_event("shutdown")
async def shutdown_event():
    """–ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –ø—Ä–∏ –æ—Å—Ç–∞–Ω–æ–≤–∫–µ"""
    global embedding_server
    if embedding_server:
        await embedding_server.shutdown()

# === OpenAI-—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ API endpoints ===

@app.post("/v1/embeddings", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    """
    –°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (OpenAI API —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π endpoint)
    
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞–∫ –æ–¥–∏–Ω–æ—á–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏, —Ç–∞–∫ –∏ –º–∞—Å—Å–∏–≤—ã —Å—Ç—Ä–æ–∫
    """
    global embedding_server
    
    if not embedding_server:
        raise HTTPException(status_code=503, detail="Embedding server –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    try:
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if isinstance(request.input, str):
            texts = [request.input]
        else:
            texts = request.input
        
        if not texts:
            raise HTTPException(status_code=400, detail="–ü—É—Å—Ç–æ–π input")
        
        logger.info(f"üì• –ó–∞–ø—Ä–æ—Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings_data = []
        total_tokens = 0
        
        for i, text in enumerate(texts):
            # –ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ)
            tokens = len(text.split())
            total_tokens += tokens
            
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
            embedding = await embedding_server.generate_embedding(text)
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Å–ø–∏—Å–æ–∫
            embedding_list = embedding.tolist()
            
            embeddings_data.append(EmbeddingData(
                embedding=embedding_list,
                index=i
            ))
        
        response = EmbeddingResponse(
            data=embeddings_data,
            model=request.model,
            usage=EmbeddingUsage(
                prompt_tokens=total_tokens,
                total_tokens=total_tokens
            )
        )
        
        logger.info(f"‚úÖ –í–æ–∑–≤—Ä–∞—â–µ–Ω—ã —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤")
        return response
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/models", response_model=ModelsResponse)
async def list_models():
    """–°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (OpenAI API —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π)"""
    models = [
        ModelInfo(
            id="text-embedding-ada-002",
            created=int(time.time()),
            owned_by="local"
        ),
        ModelInfo(
            id="qwen3-embedding-8b", 
            created=int(time.time()),
            owned_by="local"
        ),
        ModelInfo(
            id="text-embedding-3-small",
            created=int(time.time()),
            owned_by="local"
        ),
        ModelInfo(
            id="text-embedding-3-large", 
            created=int(time.time()),
            owned_by="local"
        )
    ]
    
    return ModelsResponse(data=models)

@app.get("/health")
async def health_check():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Å–µ—Ä–≤–µ—Ä–∞"""
    global embedding_server
    
    if not embedding_server:
        return {"status": "error", "message": "Embedding server –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω"}
    
    try:
        stats = await embedding_server.get_stats()
        return {
            "status": "healthy",
            "models_loaded": stats["models_loaded"],
            "gpu_available": torch.cuda.is_available(),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.get("/stats")
async def get_server_stats():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
    global embedding_server
    
    if not embedding_server:
        raise HTTPException(status_code=503, detail="Embedding server –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    
    try:
        return await embedding_server.get_stats()
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# === –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ endpoints –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ ===

@app.post("/api/embeddings", response_model=EmbeddingResponse)
async def create_embeddings_api(request: EmbeddingRequest):
    """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π endpoint –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –Ω–µ–∫–æ—Ç–æ—Ä—ã–º–∏ –∫–ª–∏–µ–Ω—Ç–∞–º–∏)"""
    return await create_embeddings(request)

@app.post("/embeddings", response_model=EmbeddingResponse)
async def create_embeddings_simple(request: EmbeddingRequest):
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π endpoint –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    return await create_embeddings(request)

@app.get("/api/models", response_model=ModelsResponse)
async def list_models_api():
    """–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π endpoint –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    return await list_models()

@app.get("/models", response_model=ModelsResponse)
async def list_models_simple():
    """–£–ø—Ä–æ—â–µ–Ω–Ω—ã–π endpoint –¥–ª—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π"""
    return await list_models()


@app.post("/search")
async def search_code(query: str, top_k: int = 5):
    """–ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
    global embedding_server
    if not embedding_server:
        raise HTTPException(status_code=503, detail="Server not ready")
    results = await embedding_server.search_index(query, top_k)
    return {"results": results}


@app.post("/generate")
async def generate(prompt: str, max_tokens: int = 256, temperature: float = 0.7):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é LLM"""
    global embedding_server
    if not embedding_server:
        raise HTTPException(status_code=503, detail="Server not ready")
    result = await embedding_server.generate_response(prompt, max_tokens, temperature)
    return result

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ —Å–µ—Ä–≤–µ—Ä–µ"""
    return {
        "name": "Cursor Compatible Embedding Server",
        "version": "1.0.0",
        "status": "running",
        "endpoints": {
            "embeddings": ["/v1/embeddings", "/api/embeddings", "/embeddings"],
            "models": ["/v1/models", "/api/models", "/models"],
            "health": "/health",
            "stats": "/stats",
            "search": "/search",
            "generate": "/generate",
            "docs": "/docs"
        },
        "port": 11435,
        "compatible_with": ["OpenAI API", "Cursor Editor", "Generic embedding clients"]
    }

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è RTX 4070
DEFAULT_CONFIG = {
    'llama_cpp_path': './llama_build/llama.cpp/build/bin/Release/llama-server.exe',
    'embedding_model_path': './models/Qwen3-Embedding-8B-Q6_K.gguf',
    'reranker_model_path': './models/Qwen3-Reranker-8B-Q6_K.gguf',
    'coder_model_path': './models/Qwen2.5-Coder-7B-Instruct.Q6_K.gguf',
    'llm_model_path': './models/Magistral-Small-2506-UD-Q4_K_XL.gguf',
    'cache_dir': './cache',
    'repo_path': '.',
    'gpu_layers': 35,  # –î–ª—è RTX 4070
    'host': '127.0.0.1',
    'port': 11435  # Cursor —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–π –ø–æ—Ä—Ç
}

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("üöÄ EMBEDDING SERVER –¥–ª—è RTX 4070 (llama.cpp)")
    print("=" * 60)
    print("üíé –ú–æ–¥–µ–ª–∏:")
    print("  ‚Ä¢ Qwen3-Embedding-8B Q6_K (4.9 GB VRAM, ~10M vectors/s)")
    print("  ‚Ä¢ Qwen3-Reranker-8B Q6_K (6 GB VRAM, ~400 pairs/s)")
    print("  ‚Ä¢ Qwen2.5-Coder-7B-Instruct Q6_K (7 GB VRAM, ~35 tok/s)")
    print("  ‚Ä¢ Magistral-Small-2506-UD-Q4_K_XL (14 GB VRAM, 6-8 tok/s)")
    print("üîç –ò–Ω–¥–µ–∫—Å—ã: FAISS-HNSW + Tantivy-BM25")
    print("üîß –î–≤–∏–∂–æ–∫: llama.cpp (–Ω–∞—Ç–∏–≤–Ω—ã–π)")
    print("=" * 60)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–∞
    server = EmbeddingServer(DEFAULT_CONFIG)
    
    try:
        await server.initialize()
        
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π
        print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –§–£–ù–ö–¶–ò–ô")
        print("-" * 30)
        
        # –¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        try:
            test_text = "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"
            embedding = await server.generate_embedding(test_text)
            print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {embedding.shape}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
        
        # –¢–µ—Å—Ç —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        try:
            query = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
            docs = [
                "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
                "Python - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", 
                "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –ò–ò"
            ]
            ranked = await server.rerank_documents(query, docs)
            print(f"‚úÖ –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(ranked)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        
        # –¢–µ—Å—Ç LLM
        try:
            prompt = "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:"
            response = await server.generate_response(prompt, max_tokens=100)
            print(f"‚úÖ LLM –æ—Ç–≤–µ—Ç: {response['tokens_per_second']:.1f} tok/s")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = await server.get_stats()
        print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"  –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {stats['models_loaded']}")
        if 'gpu_memory' in stats:
            gpu = stats['gpu_memory']
            print(f"  GPU –ø–∞–º—è—Ç—å: {gpu['allocated_gb']:.1f}/{gpu['total_gb']:.1f} GB")
        
        print("\nüéâ Embedding Server –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
        print("üí° –î–ª—è –∑–∞–ø—É—Å–∫–∞ HTTP —Å–µ—Ä–≤–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ: python embedding_server.py server")
        print("üí° –ò–ª–∏: uvicorn embedding_server:app --host 127.0.0.1 --port 11435")
        
    finally:
        await server.shutdown()


def start_server():
    """–ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        print("üöÄ –ó–∞–ø—É—Å–∫ Cursor Compatible Embedding Server...")
        print("üåê –ê–¥—Ä–µ—Å: http://127.0.0.1:11435")
        print("üìñ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://127.0.0.1:11435/docs")
        print("üí° –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
    except UnicodeEncodeError:
        # Fallback –¥–ª—è Windows –∫–æ–Ω—Å–æ–ª–∏
        print("* –ó–∞–ø—É—Å–∫ Cursor Compatible Embedding Server...")
        print("* –ê–¥—Ä–µ—Å: http://127.0.0.1:11435")
        print("* –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è: http://127.0.0.1:11435/docs")
        print("* –î–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–∞–∂–º–∏—Ç–µ Ctrl+C")
    
    uvicorn.run(
        "embedding_server:app",
        host="127.0.0.1",
        port=11435,
        reload=False,
        access_log=True,
        log_level="info"
    )

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "server":
        # –ó–∞–ø—É—Å–∫ HTTP —Å–µ—Ä–≤–µ—Ä–∞
        start_server()
    else:
        # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π
        asyncio.run(main()) 
