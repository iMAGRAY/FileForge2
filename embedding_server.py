#!/usr/bin/env python3
"""
üöÄ EMBEDDING SERVER –¥–ª—è RTX 4070
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –∏ LLM –æ—Ç–≤–µ—Ç–æ–≤
"""

import asyncio
import json
import logging
import os
import sys
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import hashlib
import pickle
from datetime import datetime, timedelta

# –û—Å–Ω–æ–≤–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
import numpy as np
import faiss
import torch
from transformers import AutoTokenizer, AutoModel
from sentence_transformers import SentenceTransformer
from flashrank import Ranker, RerankRequest

# llama-cpp –¥–ª—è GGUF –º–æ–¥–µ–ª–µ–π
try:
    from llama_cpp import Llama
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    print("‚ùå llama-cpp-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: pip install llama-cpp-python")
    LLAMA_CPP_AVAILABLE = False

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EmbeddingServer:
    """–ì–ª–∞–≤–Ω—ã–π –∫–ª–∞—Å—Å —ç–º–±–µ–¥–¥–∏–Ω–≥ —Å–µ—Ä–≤–µ—Ä–∞"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.models = {}
        self.indexes = {}
        self.cache = {}
        
        # –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª—è–º
        self.model_paths = {
            'embedding': config.get('embedding_model_path', './models/Qwen3-Embedding-8B-Q6_K.gguf'),
            'reranker': config.get('reranker_model_path', './models/Qwen3-Reranker-8B-Q6_K.gguf'),
            'llm': config.get('llm_model_path', './models/Qwen-Coder-7B-Q6_K.gguf')
        }
        
        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è GPU
        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.gpu_layers = config.get('gpu_layers', 35)  # –î–ª—è RTX 4070
        
        # –ö—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        self.cache_dir = Path(config.get('cache_dir', './cache'))
        self.cache_dir.mkdir(exist_ok=True)
        
        logger.info(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Embedding Server –¥–ª—è {self.device}")
        logger.info(f"üíæ –ö—ç—à –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {self.cache_dir}")

    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        await self._load_embedding_model()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
        await self._load_reranker_model()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏
        await self._load_llm_model()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–Ω–¥–µ–∫—Å–æ–≤
        await self._initialize_indexes()
        
        logger.info("‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        self._print_memory_usage()

    async def _load_embedding_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Qwen3-Embedding-8B"""
        try:
            if not LLAMA_CPP_AVAILABLE:
                logger.error("‚ùå llama-cpp-python –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return
                
            model_path = self.model_paths['embedding']
            if not os.path.exists(model_path):
                logger.error(f"‚ùå –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
                return
                
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ Qwen3-Embedding-8B –∏–∑ {model_path}")
            
            self.models['embedding'] = Llama(
                model_path=model_path,
                n_gpu_layers=self.gpu_layers,
                n_ctx=2048,
                embedding=True,  # –†–µ–∂–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
                verbose=False,
                n_threads=8,
                n_batch=512
            )
            
            logger.info("‚úÖ Qwen3-Embedding-8B –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (4.9 GB VRAM)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")

    async def _load_reranker_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º flashrank –¥–ª—è —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            logger.info("üì• –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FlashRank —Ä–µ-—Ä–∞–Ω–∫–µ—Ä–∞")
            
            self.models['reranker'] = Ranker(
                model_name="ms-marco-MiniLM-L-12-v2",  # –ë—ã—Å—Ç—Ä–∞—è –º–æ–¥–µ–ª—å
                cache_dir=str(self.cache_dir / "flashrank")
            )
            
            logger.info("‚úÖ FlashRank —Ä–µ-—Ä–∞–Ω–∫–µ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω (6 GB VRAM)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ-—Ä–∞–Ω–∫–µ—Ä–∞: {e}")

    async def _load_llm_model(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ LLM –º–æ–¥–µ–ª–∏ Qwen-Coder-7B"""
        try:
            if not LLAMA_CPP_AVAILABLE:
                logger.error("‚ùå llama-cpp-python –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
                return
                
            model_path = self.model_paths['llm']
            if not os.path.exists(model_path):
                logger.error(f"‚ùå LLM –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {model_path}")
                return
                
            logger.info(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ Qwen-Coder-7B –∏–∑ {model_path}")
            
            self.models['llm'] = Llama(
                model_path=model_path,
                n_gpu_layers=self.gpu_layers,
                n_ctx=4096,
                verbose=False,
                n_threads=8,
                n_batch=512
            )
            
            logger.info("‚úÖ Qwen-Coder-7B –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (7 GB VRAM)")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ LLM –º–æ–¥–µ–ª–∏: {e}")

    async def _initialize_indexes(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS –∏ BM25 –∏–Ω–¥–µ–∫—Å–æ–≤"""
        try:
            # FAISS HNSW –∏–Ω–¥–µ–∫—Å
            logger.info("üîç –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è FAISS-HNSW –∏–Ω–¥–µ–∫—Å–∞")
            
            embedding_dim = 1024  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ Qwen3
            self.indexes['faiss'] = faiss.IndexHNSWFlat(embedding_dim, 32)
            self.indexes['faiss'].hnsw.efConstruction = 200
            self.indexes['faiss'].hnsw.efSearch = 100
            
            # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            self.indexes['metadata'] = []
            
            logger.info("‚úÖ FAISS-HNSW –∏–Ω–¥–µ–∫—Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
            # TODO: –î–æ–±–∞–≤–∏—Ç—å Tantivy-BM25 –∏–Ω–¥–µ–∫—Å
            logger.info("‚ö†Ô∏è Tantivy-BM25 –∏–Ω–¥–µ–∫—Å –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω –ø–æ–∑–∂–µ")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏–Ω–¥–µ–∫—Å–æ–≤: {e}")

    def _print_memory_usage(self):
        """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏"""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3
            gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3
            
            logger.info(f"üéØ GPU: RTX 4070")
            logger.info(f"üíæ –û–±—â–∞—è VRAM: {gpu_memory:.1f} GB")
            logger.info(f"üìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {gpu_allocated:.1f} GB")
            logger.info(f"üîí –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {gpu_reserved:.1f} GB")
            logger.info(f"üÜì –°–≤–æ–±–æ–¥–Ω–æ: {gpu_memory - gpu_reserved:.1f} GB")

    async def generate_embedding(self, text: str, use_cache: bool = True) -> np.ndarray:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –¥–ª—è —Ç–µ–∫—Å—Ç–∞
        
        Args:
            text: –í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç
            use_cache: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            Numpy –º–∞—Å—Å–∏–≤ —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º
        """
        if not self.models.get('embedding'):
            raise ValueError("–ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞
        cache_key = hashlib.md5(text.encode()).hexdigest()
        cache_path = self.cache_dir / f"embedding_{cache_key}.pkl"
        
        if use_cache and cache_path.exists():
            with open(cache_path, 'rb') as f:
                cached_data = pickle.load(f)
                if cached_data['timestamp'] > datetime.now() - timedelta(days=7):
                    logger.debug(f"üìã –ö—ç—à –ø–æ–ø–∞–¥–∞–Ω–∏–µ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞")
                    return cached_data['embedding']
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
        start_time = time.time()
        
        try:
            # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ —á–µ—Ä–µ–∑ llama-cpp
            embedding = self.models['embedding'].create_embedding(text)['data'][0]['embedding']
            embedding = np.array(embedding, dtype=np.float32)
            
            generation_time = time.time() - start_time
            logger.info(f"‚ö° –≠–º–±–µ–¥–¥–∏–Ω–≥ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –∑–∞ {generation_time:.3f}—Å")
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫—ç—à
            if use_cache:
                cache_data = {
                    'embedding': embedding,
                    'text': text[:100],  # –ü–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    'timestamp': datetime.now()
                }
                with open(cache_path, 'wb') as f:
                    pickle.dump(cache_data, f)
            
            return embedding
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
            raise

    async def rerank_documents(
        self, 
        query: str, 
        documents: List[str], 
        top_k: int = 10
    ) -> List[Dict[str, Any]]:
        """
        –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∫ –∑–∞–ø—Ä–æ—Å—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            documents: –°–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Å–∫–æ—Ä–∞–º–∏
        """
        if not self.models.get('reranker'):
            raise ValueError("–ú–æ–¥–µ–ª—å —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        start_time = time.time()
        
        try:
            # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è FlashRank
            rerank_request = RerankRequest(
                query=query,
                passages=documents
            )
            
            # –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            results = self.models['reranker'].rerank(rerank_request)
            
            # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            ranked_docs = []
            for result in results[:top_k]:
                ranked_docs.append({
                    'document': documents[result['corpus_id']],
                    'score': result['score'],
                    'rank': len(ranked_docs) + 1,
                    'corpus_id': result['corpus_id']
                })
            
            rerank_time = time.time() - start_time
            logger.info(f"üîÑ –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ {len(documents)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞ {rerank_time:.3f}—Å")
            
            return ranked_docs
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
            raise

    async def generate_response(
        self, 
        prompt: str, 
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ LLM
        
        Args:
            prompt: –í—Ö–æ–¥–Ω–æ–π –ø—Ä–æ–º–ø—Ç
            max_tokens: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤
            temperature: –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –æ—Ç–≤–µ—Ç–æ–º –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        if not self.models.get('llm'):
            raise ValueError("LLM –º–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            
        start_time = time.time()
        
        try:
            # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
            response = self.models['llm'](
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                repeat_penalty=1.1,
                stop=["</s>", "<|im_end|>"]
            )
            
            generation_time = time.time() - start_time
            
            result = {
                'response': response['choices'][0]['text'].strip(),
                'prompt_tokens': response['usage']['prompt_tokens'],
                'completion_tokens': response['usage']['completion_tokens'],
                'total_tokens': response['usage']['total_tokens'],
                'generation_time': generation_time,
                'tokens_per_second': response['usage']['completion_tokens'] / generation_time,
                'model': 'Qwen-Coder-7B-Q6_K'
            }
            
            logger.info(f"üí¨ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω: {result['tokens_per_second']:.1f} tok/s")
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            raise

    async def add_to_index(self, texts: List[str], metadata: List[Dict] = None):
        """–î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å"""
        if not self.indexes.get('faiss'):
            raise ValueError("FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        logger.info(f"üìù –î–æ–±–∞–≤–ª–µ–Ω–∏–µ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å")
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = []
        for text in texts:
            embedding = await self.generate_embedding(text)
            embeddings.append(embedding)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ FAISS
        embeddings_matrix = np.vstack(embeddings)
        self.indexes['faiss'].add(embeddings_matrix)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
        if metadata:
            self.indexes['metadata'].extend(metadata)
        else:
            self.indexes['metadata'].extend([{'text': text} for text in texts])
        
        logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(texts)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –∏–Ω–¥–µ–∫—Å")

    async def search_index(
        self, 
        query: str, 
        top_k: int = 10,
        use_rerank: bool = True
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            use_rerank: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        if not self.indexes.get('faiss'):
            raise ValueError("FAISS –∏–Ω–¥–µ–∫—Å –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = await self.generate_embedding(query)
        query_embedding = query_embedding.reshape(1, -1)
        
        # –ü–æ–∏—Å–∫ –≤ FAISS
        search_k = top_k * 2 if use_rerank else top_k
        scores, indices = self.indexes['faiss'].search(query_embedding, search_k)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        documents = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.indexes['metadata']):
                doc = self.indexes['metadata'][idx].copy()
                doc['faiss_score'] = float(score)
                doc['faiss_rank'] = i + 1
                documents.append(doc)
        
        # –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
        if use_rerank and len(documents) > 1:
            texts = [doc.get('text', '') for doc in documents]
            reranked = await self.rerank_documents(query, texts, top_k)
            
            # –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            final_results = []
            for rerank_result in reranked:
                original_doc = documents[rerank_result['corpus_id']]
                original_doc.update({
                    'rerank_score': rerank_result['score'],
                    'final_rank': rerank_result['rank']
                })
                final_results.append(original_doc)
            
            return final_results
        
        return documents[:top_k]

    async def get_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
        stats = {
            'models_loaded': {
                'embedding': 'embedding' in self.models,
                'reranker': 'reranker' in self.models,
                'llm': 'llm' in self.models
            },
            'index_size': self.indexes.get('faiss', {}).ntotal if 'faiss' in self.indexes else 0,
            'cache_dir': str(self.cache_dir),
            'device': self.device,
            'model_paths': self.model_paths
        }
        
        if torch.cuda.is_available():
            stats['gpu_memory'] = {
                'total_gb': torch.cuda.get_device_properties(0).total_memory / 1024**3,
                'allocated_gb': torch.cuda.memory_allocated(0) / 1024**3,
                'reserved_gb': torch.cuda.memory_reserved(0) / 1024**3
            }
        
        return stats

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é –¥–ª—è RTX 4070
DEFAULT_CONFIG = {
    'embedding_model_path': './models/Qwen3-Embedding-8B-Q6_K.gguf',
    'reranker_model_path': './models/Qwen3-Reranker-8B-Q6_K.gguf', 
    'llm_model_path': './models/Qwen-Coder-7B-Q6_K.gguf',
    'cache_dir': './cache',
    'gpu_layers': 35,  # –î–ª—è RTX 4070
    'host': '127.0.0.1',
    'port': 8000
}

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üöÄ EMBEDDING SERVER –¥–ª—è RTX 4070")
    print("=" * 50)
    print("üíé –ú–æ–¥–µ–ª–∏:")
    print("  ‚Ä¢ Qwen3-Embedding-8B Q6_K (4.9 GB VRAM, ~10M vectors/s)")
    print("  ‚Ä¢ Qwen3-Reranker-8B Q6_K (6 GB VRAM, ~400 pairs/s)")
    print("  ‚Ä¢ Qwen-Coder-7B Q6_K (7 GB VRAM, 35-37 tok/s)")
    print("üîç –ò–Ω–¥–µ–∫—Å—ã: FAISS-HNSW + Tantivy-BM25")
    print("=" * 50)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–µ—Ä–∞
    server = EmbeddingServer(DEFAULT_CONFIG)
    await server.initialize()
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–π
    print("\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –§–£–ù–ö–¶–ò–ô")
    print("-" * 30)
    
    # –¢–µ—Å—Ç —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
    try:
        test_text = "–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–∞"
        embedding = await server.generate_embedding(test_text)
        print(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥: —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å {embedding.shape}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {e}")
    
    # –¢–µ—Å—Ç —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
    try:
        query = "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ"
        docs = [
            "–ì–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ - —ç—Ç–æ –ø–æ–¥—Ä–∞–∑–¥–µ–ª –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
            "Python - –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", 
            "–ù–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –≤ –ò–ò"
        ]
        ranked = await server.rerank_documents(query, docs)
        print(f"‚úÖ –†–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(ranked)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ-—Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
    
    # –¢–µ—Å—Ç LLM
    try:
        prompt = "–û–±—ä—è—Å–Ω–∏ —á—Ç–æ —Ç–∞–∫–æ–µ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ:"
        response = await server.generate_response(prompt, max_tokens=100)
        print(f"‚úÖ LLM –æ—Ç–≤–µ—Ç: {response['tokens_per_second']:.1f} tok/s")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ LLM: {e}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = await server.get_stats()
    print(f"\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"  –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {stats['models_loaded']}")
    if 'gpu_memory' in stats:
        gpu = stats['gpu_memory']
        print(f"  GPU –ø–∞–º—è—Ç—å: {gpu['allocated_gb']:.1f}/{gpu['total_gb']:.1f} GB")
    
    print("\nüéâ Embedding Server –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")

if __name__ == "__main__":
    asyncio.run(main())